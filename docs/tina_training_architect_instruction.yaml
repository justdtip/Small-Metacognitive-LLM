title: Tina Small-Scale Introspective LLM â€” Training Architect Notes
version: 0.1
goals:
  - preserve base model; add heads/adapters only
  - learn plan, dynamic think budget, and calibrated confidence
  - keep hidden CoT off by default; zero leakage
  - maintain serve/eval parity (stop rules, slack, tokenization)
phases:
  - name: A
    freeze_base: true
    train_adapters: true
    train_heads: true
    use_on_policy: false
  - name: B
    freeze_base: true
    train_adapters: true
    train_heads: true
    use_on_policy: true
  - name: C
    freeze_base: false
    train_adapters: true
    train_heads: true
    train_base_top: true
    use_on_policy: true
telemetry:
  - parity_digest
  - alpha_summary
  - plan_agreement
  - gate_activity, gate_coverage
artifacts:
  calibration: artifacts/metacog_calibration.json
  budget_sweep: artifacts/budget_sweep_*.{json,csv}
self_play_guidelines:
  when_to_run: |
    Use self-play (AZR) to complement supervised training once supervised losses stabilize and calibration is in place.
    Self-play can bootstrap additional reasoning diversity without new labeled data.
  hardware: |
    The safe executor runs code in a CPU subprocess with a timeout. Expect extra CPU usage and minor overhead.
    Keep GPU reserved for the model; consider adjusting steps/iters per phase based on available cores.
  seeding_buffers: |
    Buffers can start empty; for faster convergence, seed with a few trivial identity programs (e.g., add, multiply).
  monitoring: |
    Track learnability, solver accuracy, format penalty rate, and buffer sizes per mode.
    Use train/metrics.self_play_report and per-iteration stats from SelfPlayAZRTrainer.
