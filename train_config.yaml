model:
  base: model/Base
  adapter:
    path: model/Tina/checkpoint-2000  # optional; remove if no PEFT adapter is desired

data:
  dataset_name: jsonl
  jsonl: data/train.jsonl
  limit_train: 100000
  batch_size: 8

trainer:
  enabled: true
  mode: supervised
  dashboard: false
  save_interval: 1000
  eval_interval: 200

# Hooks read at top level by runner
save_interval: 1000
eval_interval: 200
sample_every: 200       # on-policy decode cadence
budget_cap: 64          # think budget soft cap (slack applies)

optim:
  wd: 0.01
  clip: 1.0
  heads_lr: 5e-4
  adapters_lr: 2e-4
  base_top_lr: 5e-5

lm_adaptation:
  enabled: true
  mode: ln_only          # or: lora (requires PEFT)
  last_k_layers: 2
  apply_to_all_layers: false
  lr_scale: 0.05

metacog:
  linked_all_layers: true
  proj_dim: 128
  agg: attn
  num_experts: 3
  feedback: true
  dump_per_layer: true

phases:
  - freeze_base: false
    train_adapters: true
    train_heads: true
    use_on_policy: true
